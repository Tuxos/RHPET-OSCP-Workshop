= First steps in OpenShift 3.9
Dennis Deitermann (dennis@redhat.com)
:scrollbar:
:data-uri:
:toc: left
:numbered:
:icons: font

image::http://www.rhpet.de/pictures/OpenShift-Workshop-banner.png[OpenShift-Workshop]

== Introduction

In this Lab you will get a hands-on introduction to the OpenShift Container Platform. You will learn to use the CLI, the Webinterface and how to deploy an application.

Please see <<Appendix: OpenShift Introduction>> if you are not aware of the architecture and components of OpenShift. *You should read this first if you are not familiar with OpenShift!*

Please have a look into the <<Appendix: Lab Environment Information>> for more information about the Lab environment, like passwords, IP addresses and so on.

=== How to access the Lab Environment

Login into the ssh gateway with the user `root` and the for the password please ask the instructor.

Your SSH Gateway: `workstation-<GUID>.generic.opentlc.com`

----
[user@host ~]$ ssh root@workstation-<GUID>.generic.opentlc.com

The authenticity of host '129.213.15.244 (129.213.15.244)' can't be established.
ECDSA key fingerprint is SHA256:ztRJAqdV6iu+yRrrP42PaGm107SEMPdYVjGXZuL3EAo.
ECDSA key fingerprint is MD5:23:22:d1:14:fa:1d:84:29:ea:dd:02:9f:94:b8:a8:87.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '129.213.15.244' (ECDSA) to the list of known hosts.
root@129.213.15.244's password: 
Last failed login: Fri Jun  1 15:29:21 EDT 2018 from 63.24.140.28 on ssh:notty
There were 8 failed login attempts since the last successful login.
Last login: Tue May 29 15:17:13 2018 from 44.234.187.19
#####################################################################################
##                                                                                 ##
##          Welcome to Red Hat Openshift Container Platform 3.9 workshop           ##
##                                                                                 ##
#####################################################################################
Information about Your current environment:

Your GUID: <GUID>
OCP WEB UI access via IP: https://masterprod-<GUID>.generic.opentlc.com:8443
Wildcard FQDN for apps: *.apps-<GUID>.generic.opentlc.com

[root@workstation-<GUID> ~]# 
----

==== Accessing the OpenShift Web Console

You can connect to the OpenShift web UI by clicking the following links:
masterprod-<GUID>.generic.opentlc.com[masterprod-<GUID>.generic.opentlc.com^]
masterdev-<GUID>.generic.opentlc.com[masterdev-<GUID>.generic.opentlc.com^]

Please accept the self-signed certificate.

If you want to know if your Webbrowser is supported by OpenShift, please have a look at https://access.redhat.com/articles/2176281[OpenShift Container Platform Tested Integrations^].

== Getting started with the OpenShift CLI and the Webinterface

Using the OpenShift Container Platform command line interface (CLI), you can create applications and manage OpenShift Container Platform projects from a terminal. The CLI is ideal in situations where you are:

* Working directly with project source code.

* Scripting OpenShift Container Platform operations.

* Restricted by bandwidth resources and cannot use the web console.

The CLI is available using the `oc` command:
----
$ oc <command>
----

=== Basic Setup and Login

The `oc login` command is the best way to initially set up the CLI, and it serves as the entry point for most users. The interactive flow helps you establish a session to an OpenShift Container Platform server with the provided credentials. The information is automatically saved in a CLI configuration file that is then used for subsequent commands.

Login into OpenShift Prod environment as `admin` user with the password `openshift`:
----
[root@workstation-<GUID> ~]# oc login https://master-prod.example.com:8443

The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Authentication required for https://master-prod.example.com:8443 (openshift)
Username: admin
Password: openshift
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
----

You will notice a lot of projects already. There will be less projects in a freshly installed OpenShift environment.

You can log out of the CLI using the `oc logout` command. But we don't do this now.

NOTE: You are running commands as `root` in this lab environment. It is unusual to use the `oc` command as root. It is common to install `oc` on your workstation or notebook. You can get the OpenShift client tools for your operating system https://docs.openshift.com/container-platform/3.9/cli_reference/get_started_cli.html[here^].

=== Projects

A project in OpenShift Container Platform contains multiple objects to make up a logical application.

Most oc commands run in the context of a project. The `oc login` selects a default project during initial setup to be used with subsequent commands. Use the following command to display the project currently in use:

----
[root@workstation-<GUID>~]# oc project

Using project "default" on server "https://master-prod.example.com:8443".
----

If you have access to multiple projects, use the following syntax to switch between projects by specifying the project name:
----
[root@workstation-<GUID>~]# oc project default

Already on project "default" on server "https://master-prod.example.com:8443".
----

The `oc status` command shows a high level overview of the project currently in use, with its components and their relationships, as shown in the following example:
----
[root@workstation-<GUID> ~]# oc status

In project default on server https://master-prod.example.com:8443

https://docker-registry-default.apps-<GUID>.generic.opentlc.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.9.14 
    deployment #1 deployed 8 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443->8443, 53->8053, 53->8053

http://registry-console-default.apps-<GUID>.generic.opentlc.com to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:v3.9 
    deployment #1 deployed 8 weeks ago - 1 pod

svc/router - 172.30.38.74 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.9.14 
    deployment #1 deployed 8 weeks ago - 1 pod

3 warnings identified, use 'oc status -v' to see details.
----

If you want to learn more about the `oc` command, please look at the documentation: +
https://docs.openshift.com/container-platform/3.9/cli_reference/basic_cli_operations.html[Developer CLI Operations^] +
https://docs.openshift.com/container-platform/3.9/cli_reference/admin_cli_operations.html[Administrator CLI Operations^]

=== Verify Your Environment

Run `oc get nodes` to check the status of your OpenShift Prod environment hosts:
----
[root@workstation-<GUID>~]# oc get nodes

NAME                     STATUS    ROLES     AGE       VERSION
master-prod.example.com   Ready     master    56d       v1.9.1+a0ce1bc657
node01prod.example.com    Ready     compute   56d       v1.9.1+a0ce1bc657
node02prod.example.com    Ready     compute   56d       v1.9.1+a0ce1bc657
node03prod.example.com    Ready     compute   56d       v1.9.1+a0ce1bc657
----

Check with `oc get pods` if the installer has deployed the router and the registry containers:
----
[root@workstation-<GUID>~]# oc get pods

NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-gjm5v    1/1       Running   21         56d
registry-console-1-gwdpv   1/1       Running   20         56d
router-1-pdrms             1/1       Running   22         56d
----

=== Configure OpenShift

In this section, you check the labels and do some intial configuration.

==== Labels

Labels are used to organize, group, or select API objects. For example, pods are "tagged" with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.

Most objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.

Labels are simple key/value pairs, as in the following example:
----
labels:
  key1: value1
  key2: value2
----

Consider:

* A pod consisting of an *nginx* container, with the label *role=webserver*.

* A pod consisting of an *Apache httpd* container, with the same label *role=webserver*.

A service or replication controller that is defined to use pods with the *role=webserver* label treats both of these pods as part of the same group.

==== Check Regions and Zones

We already labeled your nodes.

Check the labels of the nodes:
----
[root@workstation-<GUID> ~]# oc get nodes --show-labels

NAME                      STATUS    ROLES     AGE       VERSION             LABELS
master-prod.example.com   Ready     master    58d       v1.9.1+a0ce1bc657   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master-prod.example.com,node-role.kubernetes.io/master=true,openshift-infra=apiserver,region=infra,zone=default
node01prod.example.com    Ready     compute   58d       v1.9.1+a0ce1bc657   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node01prod.example.com,node-role.kubernetes.io/compute=true,region=primary,zone=main
node02prod.example.com    Ready     compute   58d       v1.9.1+a0ce1bc657   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node02prod.example.com,node-role.kubernetes.io/compute=true,region=primary,zone=main
node03prod.example.com    Ready     compute   58d       v1.9.1+a0ce1bc657   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node03prod.example.com,node-role.kubernetes.io/compute=true,region=primary,zone=main
----

You have a running OpenShift environment across four hosts with one master and three nodes, divided into two regions: infra and primary and two zones: default and main.

Check where the registry, registry-console and router is running:
[NOTE]
In a production OpenShift environment you will have dedicated infrastructure nodes so you will not run infrastructure components on the master or app nodes.
----
[root@workstation-<GUID>~]# oc get pods -o wide

NAME                       READY     STATUS    RESTARTS   AGE       IP             NODE
docker-registry-1-gjm5v    1/1       Running   21         56d       10.128.0.117   master-prod.example.com
registry-console-1-gwdpv   1/1       Running   20         56d       10.129.0.107   node02prod.example.com
router-1-pdrms             1/1       Running   22         56d       192.168.1.10   master-prod.example.com
----

If you want all infrastructure pods are running on a special infranode, you must configure a default node selector for this.
Please have a look https://blog.openshift.com/deploying-applications-to-specific-nodes/[here^] if you want to have more information about this.

=== Registry

The Registry is a stateless, highly scalable server side application that stores and lets you distribute Container images.
OpenShift Container Platform can utilize any server implementing the Docker registry API as a source of images, including the Docker Hub, private registries run by third parties, and the integrated OpenShift Container Platform registry.

==== Integrated OpenShift Container Registry

OpenShift Container Platform provides an integrated container registry called OpenShift Container Registry (OCR) that adds the ability to automatically provision new image repositories on demand. This provides users with a built-in location for their application builds to push the resulting images.

Whenever a new image is pushed to OCR, the registry notifies OpenShift Container Platform about the new image, passing along all the information about it, such as the namespace, name, and image metadata. Different pieces of OpenShift Container Platform react to new images, creating new builds and deployments.

==== Check integrated Registry

To check the URL of the docker registry run `oc status`:
----
[root@workstation-<GUID> ~]# oc status -v

In project default on server https://master-prod.example.com:8443

https://docker-registry-default.apps-<GUID>.generic.opentlc.com (passthrough) to pod port 5000-tcp (svc/docker-registry)
  dc/docker-registry deploys docker.io/openshift3/ose-docker-registry:v3.9.14 
    deployment #1 deployed 8 weeks ago - 1 pod

svc/kubernetes - 172.30.0.1 ports 443->8443, 53->8053, 53->8053

http://registry-console-default.apps-<GUID>.generic.opentlc.com to pod port registry-console (svc/registry-console)
  dc/registry-console deploys registry.access.redhat.com/openshift3/registry-console:v3.9 
    deployment #1 deployed 8 weeks ago - 1 pod

svc/router - 172.30.38.74 ports 80, 443, 1936
  dc/router deploys docker.io/openshift3/ose-haproxy-router:v3.9.14 
    deployment #1 deployed 8 weeks ago - 1 pod

Warnings:
  * pod/docker-registry-1-97k7b has restarted 22 times
  * pod/registry-console-1-sffxp has restarted 21 times
  * pod/router-1-5qs7m has restarted 22 times

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
----

Everything seems fine (you can ignore the warnings at this time ;-).

=== Resource Management Lab

In this lab, you learn how to manage OpenShift Container Platform resources.

* *Manage Users, Projects, and Quotas*
+
In this section, you create projects and test the use of quotas and limits.

* *Create Services and Routes*
+
In this section, you manually create services and routes for pods and review the changes to a service when scaling an application.

* *Explore Containers*
+
In this section, you run commands within active pods and explore the `docker-registry` and `Default Router` containers.

==== Manage Users, Projects, and Quotas

===== Create Project

On the master host, run `oc adm` to create and assign the administrative user `student` to the project:
----
[root@workstation-<GUID>~]# oc adm new-project resourcemanagement --display-name="Resources Management" --description="This is the project we use to learn about resource management" --admin=student  --node-selector='region=primary'

Created project resourcemanagement
----

[NOTE]
`student` can create his own project with the `oc new-project` command, an option you will experiment with later in this course. Note that defining the `--node-selector` is optional.

==== View Resources in Web Console

Now have a look at the web console.

. Open your web browser and go to the http://seats.rpet.de[Seats to GUID page] and click on your OpenShift Webinterface link.
+
[WARNING]
====
Please get sure that the Link the the one from *your* Seat ID.
====

. When prompted, type the username and password, as follows:
** *Username*: `student`
** *Password*: `openshift`

. In the web console, click the *Resources Management* project link in the top right corner unter `My Projects`.
+
[NOTE]
The project is empty because it has no apps. You change that as part of this lab. 

===== Apply Quota to Project

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.

. Create a quota definition file:
+
----
[root@workstation-<GUID>~]# cat << EOF > quota.json
{
  "apiVersion": "v1",
  "kind": "ResourceQuota",
  "metadata": {
    "name": "test-quota"
  },
  "spec": {
    "hard": {
      "memory": "512Mi",
      "cpu": "20",
      "pods": "3",
      "services": "5",
      "replicationcontrollers":"5",
      "resourcequotas":"1"
    }
  }
}
EOF
----

. Run `oc create` to apply the file you just created:
+
----
[root@workstation-<GUID>~]# oc create -f quota.json --namespace=resourcemanagement

resourcequota "test-quota" created
----

.. Verify that the quota exists:
+
----
[root@workstation-<GUID>~]# oc get quota -n resourcemanagement

NAME         AGE
test-quota   47s
----

.. Verify the limits and examine the usage:
+
[tabsize=8]
----
[root@workstation-<GUID>~]# oc describe quota test-quota -n resourcemanagement

Name:                   test-quota
Namespace:              resourcemanagement
Resource                Used  Hard
--------                ----  ----
cpu                     0     20
memory                  0     512Mi
pods                    0     3
replicationcontrollers  0     5
resourcequotas          1     1
services                0     5
----
+

. On the web console, click the *Resource Management* project.

. Go to the *Resources* tab

. Click *Quota* for information about the quota set.

==== Apply Limit Ranges to Project

For quotas to be effective, you must create _limit ranges_. They allocate the maximum, minimum, and default memory and CPU at both the pod and container level. Deployments to projects with a quota set will fail, if there are no default limits set for containers and pods. Pod and Containers with no limits are called unbound and are forbidden to run in quota projects.

. Create the `limits.json` file:
+
----
[root@workstation-<GUID>~]# cat << EOF > limits.json
{
    "kind": "LimitRange",
    "apiVersion": "v1",
    "metadata": {
        "name": "limits",
        "creationTimestamp": null
    },
    "spec": {
        "limits": [
            {
                "type": "Pod",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                }
            },
            {
                "type": "Container",
                "max": {
                    "cpu": "500m",
                    "memory": "750Mi"
                },
                "min": {
                    "cpu": "10m",
                    "memory": "5Mi"
                },
                "default": {
                    "cpu": "100m",
                    "memory": "100Mi"
                }
            }
        ]
    }
}
EOF
----

. Run `oc create` against the `limits.json` file and the
 `resourcemanagement` project:
+
----
[root@workstation-<GUID>~]# oc create -f limits.json --namespace=resourcemanagement

limitrange "limits" created
----

. Review your limit ranges:
+
----
[root@workstation-<GUID>~]# oc describe limitranges limits -n resourcemanagement

Name:       limits
Namespace:  resourcemanagement
Type        Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---    ---------------  -------------  -----------------------
Pod         memory    5Mi  750Mi  -                -              -
Pod         cpu       10m  500m   -                -              -
Container   cpu       10m  500m   100m             100m           -
Container   memory    5Mi  750Mi  100Mi            100Mi          -
----

==== Test Quota and Limit Settings

. Now we switch to the the OpenShift user `student`. 

.. When prompted, type the username and password:
** *Username*: `student`
** *Password*: `openshift`
+
----
[root@workstation-<GUID>~]$ oc login https://master-prod.example.com:8443 -u student
----

* The output is as follows:
+
----
Login successful.

You have one project on this server: "resourcemanagement"

Using project "resourcemanagement".
----
+
. Change to the project resourcemanagement if not already:
----
[root@workstation-<GUID>~]# oc project resourcemanagement

Now using project "resourcemanagement" on server "https://master-prod.example.com:8443".
----

NOTE: This lab shows you the manual, step-by-step method of creating each object. This is done only for educational purpose. There are easier ways to create deployments and all the required objects. The most powerful way to create apps on OpenShift is the `oc new-app` command, which is covered later in this lab.

. Create the `hello-pod.json` pod definition file:
----
[root@workstation-<GUID>~]$ cat <<EOF > hello-pod.json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-openshift",
    "creationTimestamp": null,
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "hello-openshift",
        "image": "openshift/hello-openshift:v1.5.1",
        "ports": [
          {
            "containerPort": 8080,
            "protocol": "TCP"
          }
        ],
        "resources": {
        },
        "terminationMessagePath": "/dev/termination-log",
        "imagePullPolicy": "IfNotPresent",
        "capabilities": {},
        "securityContext": {
          "capabilities": {},
          "privileged": false
        }
      }
    ],
    "restartPolicy": "Always",
    "dnsPolicy": "ClusterFirst",
    "serviceAccount": ""
  },
  "status": {}
}
EOF
----

===== Run Pod

Here, you create a simple pod without a _route_ or _service_:

Create and verify the `hello-openshift` pod:
----
[root@workstation-<GUID>~]$ oc create -f hello-pod.json

pod "hello-openshift" created
----
Wait a few seconds until the pod is up and running. (~20 seconds are needed) You can use `oc get pods -w` to see it directly when the status is changing.
----
[root@workstation-<GUID>~]# oc get pods

NAME              READY     STATUS    RESTARTS   AGE
hello-openshift   1/1       Running   0          20s
----

Run `oc describe` for details on your pod:
----
[root@workstation-<GUID>~]# oc describe pod hello-openshift

Name:         hello-openshift
Namespace:    resourcemanagement
Node:         node01dev.example.com/192.168.1.11
Start Time:   Fri, 01 Jun 2018 14:12:51 -0400
Labels:       name=hello-openshift
Annotations:  kubernetes.io/limit-ranger=LimitRanger plugin set: cpu, memory request for container hello-openshift; cpu, memory limit for container hello-openshift
              openshift.io/scc=restricted
Status:       Running
IP:           10.131.0.108
Containers:
  hello-openshift:
    Container ID:   docker://ee60fea0f0ff83047ba222b37fe3b9207e44a61bfe656e3b4ffe6ba17c4cd32f
    Image:          openshift/hello-openshift:v1.5.1
    Image ID:       docker-pullable://docker.io/openshift/hello-openshift@sha256:38d25a17becd423e0c6d846fc434336bb872d7be02a3357cc90fef153bc894f2
    Port:           8080/TCP
    State:          Running
      Started:      Fri, 01 Jun 2018 14:12:56 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  100Mi
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4w99t (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-4w99t:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-4w99t
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  region=primary
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              2m    default-scheduler               Successfully assigned hello-openshift to node01dev.example.com
  Normal  SuccessfulMountVolume  2m    kubelet, node01dev.example.com  MountVolume.SetUp succeeded for volume "default-token-4w99t"
  Normal  Pulling                2m    kubelet, node01dev.example.com  pulling image "openshift/hello-openshift:v1.5.1"
  Normal  Pulled                 2m    kubelet, node01dev.example.com  Successfully pulled image "openshift/hello-openshift:v1.5.1"
  Normal  Created                2m    kubelet, node01dev.example.com  Created container
  Normal  Started                2m    kubelet, node01dev.example.com  Started container
----

Test that the application in your pod is responding with `Hello OpenShift`.

First get the IP address:
----
[root@workstation-<GUID>~]# oc describe pod hello-openshift|grep IP:|awk '{print $2}'

<IP ADDRESS>
----
Then use the IP address to check that the app is doing what it should:
----
[root@workstation-<GUID>~]# ssh master-prod.example.com 'curl -s http://<IP ADDRESS>:8080'

Hello OpenShift!
----
NOTE: We must ssh into the master node, because the ssh gateway has no direct network connection to the pod network.

Delete all the objects in your `hello-pod.json` definition file, which, at this point, is the pod only:
----
[root@workstation-<GUID>~]# oc delete -f hello-pod.json

pod "hello-openshift" deleted
----

TIP: You can also delete a pod using the following command format: `oc delete pod <PODNAME>`.

Create a new definition file that launches four `hello-openshift` pods:
----
[root@workstation-<GUID>~]$ cat << EOF > hello-many-pods.json
{
  "metadata":{
    "name":"quota-pod-deployment-test"
  },
  "kind":"List",
  "apiVersion":"v1",
  "items":[
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-1",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-2",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-3",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    },
    {
      "kind": "Pod",
      "apiVersion": "v1",
      "metadata": {
        "name": "hello-openshift-4",
        "creationTimestamp": null,
        "labels": {
          "name": "hello-openshift"
        }
      },
      "spec": {
        "containers": [
          {
            "name": "hello-openshift",
            "image": "openshift/hello-openshift:v1.5.1",
            "ports": [
              {
                "containerPort": 8080,
                "protocol": "TCP"
              }
            ],
            "resources": {
              "limits": {
                "cpu": "10m",
                "memory": "16Mi"
              }
            },
            "terminationMessagePath": "/dev/termination-log",
            "imagePullPolicy": "IfNotPresent",
            "capabilities": {},
            "securityContext": {
              "capabilities": {},
              "privileged": false
            }
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst",
        "serviceAccount": ""
      },
      "status": {}
    }
  ]
}
EOF
----

Create the items in the `hello-many-pods.json` file:
----
[root@workstation-<GUID>~]# oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
Error from server (Forbidden): pods "hello-openshift-4" is forbidden: exceeded quota: test-quota, requested: pods=1, used: pods=3, limited: pods=3
----

[NOTE]
Because you defined a quota before, `oc create` created three pods only instead of four.

Delete the object in the `hello-many-pods.json` definition file:
----
[root@workstation-<GUID>~]# oc delete -f hello-many-pods.json

pod "hello-openshift-1" deleted
pod "hello-openshift-2" deleted
pod "hello-openshift-3" deleted
Error from server (NotFound): pods "hello-openshift-4" not found
----

==== Create Services and Routes

NOTE: With `oc whoami` you can check with which user you are logged in.

As `student`, create a project called `scvslab`:
----
[root@workstation-<GUID>~]# oc new-project svcslab --display-name="Services Lab" --description="This is the project we use to learn about services"

Now using project "svcslab" on server "https://master-prod.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----

Create the `hello-service.json` file:
----
[root@workstation-<GUID>~]$ cat <<EOF > hello-service.json
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-service",
    "labels": {
      "name": "hello-openshift"
    }
  },
  "spec": {
    "selector": {
      "name":"hello-openshift"
    },
    "ports": [
      {
        "protocol": "TCP",
        "port": 8888,
        "targetPort": 8080
      }
    ]
  }
}
EOF
----

Create the `hello-service` service:
----
[root@workstation-<GUID>~]$ oc create -f hello-service.json

service "hello-service" created
----

Display the services that are running in the current project:
----
[root@workstation-<GUID>~]# oc get services

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
hello-service   ClusterIP   172.30.69.150   <none>        8888/TCP   34s
----

Examine the details of your service. Note the following: +
*Selector*: Describes which pods the service selects or lists. +
*Endpoints*: Displays all the pods that are currently listed (none in your current project).

----
[root@workstation-<GUID>~]# oc describe service hello-service

Name:              hello-service
Namespace:         svcslab
Labels:            name=hello-openshift
Annotations:       <none>
Selector:          name=hello-openshift
Type:              ClusterIP
IP:                172.30.69.150
Port:              <unset>  8888/TCP
TargetPort:        8080/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
----

Create pods according to the `hello-many-pods.json` definition file:
----
[root@workstation-<GUID>~]# oc create -f hello-many-pods.json

pod "hello-openshift-1" created
pod "hello-openshift-2" created
pod "hello-openshift-3" created
pod "hello-openshift-4" created
----

Wait a few seconds and check the service again.

The pods that share the label `name=hello-openshift` are all listed:
----
[root@workstation-<GUID>~]# oc describe service hello-service

Name:              hello-service
Namespace:         svcslab
Labels:            name=hello-openshift
Annotations:       <none>
Selector:          name=hello-openshift
Type:              ClusterIP
IP:                172.30.69.150
Port:              <unset>  8888/TCP
TargetPort:        8080/TCP
Endpoints:         10.129.0.113:8080,10.130.0.97:8080,10.130.0.98:8080 + 1 more...
Session Affinity:  None
Events:            <none>
----

Test that your service is working:
----
[root@workstation-<GUID>~]# oc describe service hello-service|grep IP:|awk '{print $2}'

<IP ADDRESS>
----

Please use your hello-service IP address to check if the service is running fine:
----
[root@workstation-<GUID>~]# ssh master-prod.example.com 'curl -s http://<IP ADRESS>:8888'

Hello OpenShift!
----

==== Explore Containers and Routes

Next, take a look at the router and registry containers.

===== Create Applications As Examples

As `student`, create a project called `explore-example`:
----
[root@workstation-<GUID>~]# oc new-project explore-example --display-name="Explore Example" --description="This is the project we use to learn about connecting to pods"

Now using project "explore-example" on server "https://master-prod.example.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----

Applying the same image as before, run `oc new-app` to deploy `hello-openshift`:
----
[root@workstation-<GUID>~]# oc new-app --docker-image=openshift/hello-openshift:v1.5.1 -l "todelete=yes"

--> Found Docker image fb15b0b (12 months old) from Docker Hub for "openshift/hello-openshift:v1.5.1"

    * An image stream will be created as "hello-openshift:v1.5.1" that will track this image
    * This image will be deployed in deployment config "hello-openshift"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello-openshift"
      * Other containers can access this service through the hostname "hello-openshift"
    * WARNING: Image "openshift/hello-openshift:v1.5.1" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources with label todelete=yes ...
    imagestream "hello-openshift" created
    deploymentconfig "hello-openshift" created
    service "hello-openshift" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello-openshift' 
    Run 'oc status' to view your app.
----

Verify that `oc new-app` has created a pod and the service.
----
[root@workstation-<GUID>~]# oc get svc

NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
hello-openshift   ClusterIP   172.30.182.169   <none>        8080/TCP,8888/TCP   35s
----

Wait until the Conatiner Status is Running.
----
[root@workstation-<GUID>~]# oc get pods

NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-x6snq   1/1       Running   0          1m
----

Expose the service and create a route for the application.
*Please change `<GUID>` to your GUID.*
----
[root@workstation-<GUID>~]# oc expose service hello-openshift --hostname=explore.apps-<GUID>.generic.opentlc.com

route "hello-openshift" exposed
----

Check if the route works fine.
*Please change `<GUID>` to your GUID.*
----
[root@workstation-<GUID> ~]# curl http://explore.apps-<GUID>.generic.opentlc.com

Hello OpenShift!
----

Now it works without the ssh, because we have an external route to the container.

In a later section, you explore the `docker-registry` container. To save time, start an S2I build now to push an image into the registry:

----
[root@workstation-<GUID> ~]# oc new-app https://github.com/openshift/sinatra-example -l "todelete=yes"

--> Found image 644f40d (3 months old) in image stream "openshift/ruby" under tag "2.4" for "ruby"

    Ruby 2.4 
    -------- 
    Ruby 2.4 available as container is a base platform for building and running various Ruby 2.4 applications and frameworks. Ruby is the interpreted scripting language for quick and easy object-oriented programming. It has many features to process text files and to do system management tasks (as in Perl). It is simple, straight-forward, and extensible.

    Tags: builder, ruby, ruby24, rh-ruby24

    * The source repository appears to match: ruby
    * A source build using source code from https://github.com/openshift/sinatra-example will be created
      * The resulting image will be pushed to image stream "sinatra-example:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "sinatra-example"
    * Port 8080/tcp will be load balanced by service "sinatra-example"
      * Other containers can access this service through the hostname "sinatra-example"

--> Creating resources with label todelete=yes ...
    imagestream "sinatra-example" created
    buildconfig "sinatra-example" created
    deploymentconfig "sinatra-example" created
    service "sinatra-example" created
--> Success
    Build scheduled, use 'oc logs -f bc/sinatra-example' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/sinatra-example' 
    Run 'oc status' to view your app.
----

===== Connect to Default Router Container

Make sure to use the `default` project as `admin` user. Open a Shell into the container with `oc rsh`
 command along with the default router's pod name.
----
[root@workstation-<GUID> ~]# oc login -u admin

Authentication required for https://master-prod.example.com:8443 (openshift)
Username: admin
Password: 
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
  * explore-example
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    svcslab

Using project "explore-example".
----

----
[root@workstation-<GUID> ~]# oc project default

Now using project "default" on server "https://master-prod.example.com:8443".
----

Lets look for the pod name of the router.
----
[root@workstation-<GUID> ~]# oc get pods

NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-59t4j    1/1       Running   0          27m
registry-console-1-sr4jw   1/1       Running   0          27m
router-1-5qs7m             1/1       Running   22         58d

----

Login into your router pod.
----
[root@workstation-<GUID> ~]# oc rsh router-1-<your id>
----

This prompt is displayed:
----
sh-4.2$ 
----

You are now running `bash` inside the container.

. Do the following:
.. Run `id`.
.. Run `pwd` and `ls` and note the directory you are in.
.. Run `grep hello-openshift` on the `haproxy.config` file.
.. Run `cat haproxy.config` to have a look on your configuration file.
+
----
sh-4.2$ id

uid=1000000000 gid=0(root) groups=0(root),1000000000
----
+
----
sh-4.2$ pwd

/var/lib/haproxy/conf
----
+
----
sh-4.2$ ls

cert_config.map       haproxy-config.template  os_http_be.map		 os_route_http_redirect.map  os_wildcard_domain.map
default_pub_keys.pem  haproxy.config	       os_reencrypt.map		 os_sni_passthrough.map
error-page-503.http   os_edge_http_be.map      os_route_http_expose.map  os_tcp_be.map
----
+
----
sh-4.2$ grep hello-openshift haproxy.config 

backend be_http:explore-example:hello-openshift
  server pod:hello-openshift-1-cbj67:hello-openshift:10.128.0.32:8080 10.128.0.32:8080 cookie aec891a1968640037700c7ee813141ed weight 256 check inter 5000ms

sh-4.2$ ps -ef

UID         PID   PPID  C STIME TTY          TIME CMD
1000000+      1      0  0 12:48 ?        00:01:46 /usr/bin/openshift-router
1000000+    659      1  0 21:53 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/haproxy/conf/haproxy.config -p /var/lib/haproxy/ru
1000000+    663      0  0 21:57 ?        00:00:00 /bin/sh
1000000+    672    663  0 21:59 ?        00:00:00 ps -ef
----
.. Examine the haproxy.config more closely. This could look something like this like this:
+
[subs=+macros]
----
sh-4.2$ grep -A 40 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http:explore-example:hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
  cookie 221fb72041e5cd15282358ec8d8c82b9 insert indirect nocache httponly
  server pod:hello-openshift-1-cbj67:hello-openshift:10.128.0.32:8080 10.128.0.32:8080 cookie aec891a1968640037700c7ee813141ed weight 256 check inter 5000ms
# Secure backend, pass through
backend be_tcp:kube-service-catalog:apiserver
  balance source
  hash-type consistent
  timeout check 5000ms
  server pod:apiserver-wdzlt:apiserver:10.131.0.128:6443 10.131.0.128:6443 weight 256
# Secure backend which requires re-encryption
backend be_secure:openshift-ansible-service-broker:asb-1338
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
  cookie 89a6d633054ded194d4e1360cdc1fbef insert indirect nocache httponly secure
  server pod:asb-14-7dmvq:asb:10.128.0.30:1338 10.128.0.30:1338 cookie 00f845fa98ecc009367efd1503621909 weight 256 ssl verifyhost asb.openshift-ansible-service-broker.svc verify required ca-file /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
----
+
You see that you have only one endpoint defined. (The line which starts with `server pod`)
+
.. Exit the bash in the container to return to the root@workstation-GUID shell
+
----
sh-4.2$ exit

[root@workstation-<GUID> ~]#
----
. As `student`, scale `hello-openshift` to have five replicas of its pod:
+
----
[root@workstation-<GUID> ~]# oc login -u student
----
+
----
[root@workstation-<GUID> ~]# oc get deploymentconfig

NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
hello-openshift   1          1         1         config,image(hello-openshift:v1.5.1)
sinatra-example   1          1         1         config,image(sinatra-example:latest)
----
+
----
[root@workstation-<GUID> ~]# oc scale dc hello-openshift --replicas=5

deploymentconfig "hello-openshift" scaled
----

. As `admin` go back to the router container and view the `haproxy.config` file again:
+
[subs=+macros]
----
[root@workstation-<GUID> ~]$ oc login -u admin

...

[root@workstation-<GUID> ~]# oc project default

Now using project "default" on server "https://master-prod.example.com:8443".
----
+
Login into your router pod. Get the name with `oc get pods`.
----
[root@workstation-<GUID> ~]# oc rsh router-1-5qs7m

sh-4.2$ grep -A 70 hello-openshift haproxy.config | sed '/^ *$/d'

backend be_http:explore-example:hello-openshift
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
  cookie 221fb72041e5cd15282358ec8d8c82b9 insert indirect nocache httponly
  server pod:hello-openshift-1-cbj67:hello-openshift:10.128.0.32:8080 10.128.0.32:8080 cookie aec891a1968640037700c7ee813141ed weight 256 check inter 5000ms
  server pod:hello-openshift-1-gss9j:hello-openshift:10.129.0.43:8080 10.129.0.43:8080 cookie 7b3ce237d79ca81f1aad237c3e287c53 weight 256 check inter 5000ms
  server pod:hello-openshift-1-p249k:hello-openshift:10.129.0.44:8080 10.129.0.44:8080 cookie de43cd4369068b269cb5401ccf2f3754 weight 256 check inter 5000ms
  server pod:hello-openshift-1-bkv72:hello-openshift:10.130.0.26:8080 10.130.0.26:8080 cookie 3495ab1d4cb844eca01dcaa40a71016c weight 256 check inter 5000ms
  server pod:hello-openshift-1-wppvm:hello-openshift:10.130.0.27:8080 10.130.0.27:8080 cookie 51f51c10daaac187de4d697f4999699c weight 256 check inter 5000ms
# Secure backend, pass through
backend be_tcp:kube-service-catalog:apiserver
  balance source
  hash-type consistent
  timeout check 5000ms
  server pod:apiserver-wdzlt:apiserver:10.131.0.128:6443 10.131.0.128:6443 weight 256
# Secure backend which requires re-encryption
backend be_secure:openshift-ansible-service-broker:asb-1338
  mode http
  option redispatch
  option forwardfor
  balance leastconn
  timeout check 5000ms
  http-request set-header X-Forwarded-Host %[req.hdr(host)]
  http-request set-header X-Forwarded-Port %[dst_port]
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header Forwarded for=%[src];host=%[req.hdr(host)];proto=%[req.hdr(X-Forwarded-Proto)]
  cookie 89a6d633054ded194d4e1360cdc1fbef insert indirect nocache httponly secure
  server pod:asb-14-7dmvq:asb:10.128.0.30:1338 10.128.0.30:1338 cookie 00f845fa98ecc009367efd1503621909 weight 256 ssl verifyhost asb.openshift-ansible-service-broker.svc verify required ca-file /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
----

All of your `server pods` within the `haproxy` configuration are listed.

NOTE: Remember, the router routes proxy connections to the pods directly and not through the service. The router uses the service only to obtain a list of the pod endpoints (IP addresses).

Leave the container an switch to `student`.
----
sh-4.2$ exit

exit
[root@workstation-<GUID> ~]# oc login -u student
----

==== Explore Logs with the cli

As user `student`, check the logs of the build that we stared a while back:
----
[root@workstation-<GUID> ~]# oc logs builds/sinatra-example-1

Cloning "https://github.com/openshift/sinatra-example" ...
	Commit:	ff65a82271fffc60d4129bccde9c42ded49a199d (Merge pull request #11 from corey112358/patch-1)
	Author:	Ben Parees <bparees@users.noreply.github.com>
	Date:	Wed Jul 22 00:20:36 2015 -0400
---> Installing application source ...
---> Building your Ruby application from source ...
---> Running 'bundle install --retry 2 --deployment --without development:test' ...
Fetching gem metadata from https://rubygems.org/.........
Fetching version metadata from https://rubygems.org/.
Installing rack 1.6.0
Installing tilt 1.4.1
Using bundler 1.13.7
Installing rack-protection 1.5.3
Installing sinatra 1.4.5
Bundle complete! 1 Gemfile dependency, 5 gems now installed.
Gems in the groups development and test were not installed.
Bundled gems are installed into ./bundle.
---> Cleaning up unused ruby gems ...
Running `bundle clean   --verbose` with bundler 1.13.7
Found no changes, using resolution from the lockfile

Pushing image docker-registry.default.svc:5000/explore-example/sinatra-example:latest ...
Pushed 0/6 layers, 6% complete
Pushed 1/6 layers, 28% complete
Pushed 2/6 layers, 40% complete
Pushed 3/6 layers, 55% complete
Pushed 4/6 layers, 71% complete
Pushed 5/6 layers, 100% complete
Pushed 6/6 layers, 100% complete
Push successful
----

Notice the last few lines here. The *Push successful* indicates that the new container image was put into your internal registry.

Look with `oc get pods` for the sinatra-example-1 pod.
----
[root@workstation-<GUID> ~]# oc get pods

NAME                      READY     STATUS      RESTARTS   AGE
hello-openshift-1         1/1       Running     0          14m
hello-openshift-1-bkv72   1/1       Running     0          12m
hello-openshift-1-cbj67   1/1       Running     0          39m
hello-openshift-1-gss9j   1/1       Running     0          12m
hello-openshift-1-p249k   1/1       Running     0          12m
hello-openshift-1-wppvm   1/1       Running     0          12m
hello-openshift-2         1/1       Running     0          14m
hello-openshift-3         1/1       Running     0          14m
hello-openshift-4         1/1       Running     0          14m
sinatra-example-1-build   0/1       Completed   0          30m
sinatra-example-1-jm7nm   1/1       Running     0          29m
----

Look for the pod log with your pod name `sinatra-example-1-<your own name from oc get pods>`:
----
[root@workstation-<GUID> ~]# oc logs pods/sinatra-example-1-jm7nm

You might consider adding 'puma' into your Gemfile.
/opt/app-root/src/bundle/ruby/2.4.0/gems/sinatra-1.4.5/lib/sinatra/base.rb:1217: warning: constant ::Fixnum is deprecated
/opt/app-root/src/bundle/ruby/2.4.0/gems/sinatra-1.4.5/lib/sinatra/base.rb:1217: warning: constant ::Fixnum is deprecated
/opt/app-root/src/bundle/ruby/2.4.0/gems/sinatra-1.4.5/lib/sinatra/base.rb:1217: warning: constant ::Fixnum is deprecated
[2018-06-01 21:53:43] INFO  WEBrick 1.3.1
[2018-06-01 21:53:43] INFO  ruby 2.4.2 (2017-09-14) [x86_64-linux]
[2018-06-01 21:53:43] INFO  WEBrick::HTTPServer#start: pid=1 port=8080
----

You can also look into all logs through the OpenShift Webinterface.

=== Creating Applications Lab

This lab includes the following sections:

* *Deploy Application on Web Console*
+
In this section, you deploy an application from a code repository and follow the build logs on the OpenShift Container Platform web console and CLI.

* *Customize Build Script*

- Create an application from a forked Git repository, inject a custom build script, and start a rebuild from the web console.

- Review your custom script messages in the logs.

==== Deploy Application on Web Console

Here, you connect to and become familiar with the web console, create a project and an application, and scale a deployment.

===== Connect To and Explore Web Console

. Use your browser to go to the OpenShift web console. If not already, use the link in the http://seats.rhpet.de[Seat-to-GUID^] page.

. Log in as `student` with the password `openshift`.

. Take a few minutes to look around.

===== Create New Project

. Click the blue *Create Project* button in the top right corner.

. Give the new project a name, display name, and description:
* *Name*: `my-ruby-project`
* *Display Name*: `My Ruby Example Project`
* *Description*: An explanation of your choice
. Click on the blue *Create* button.

Once the project is in place, click on your new project `My Ruby Example Project`.

==== Create New Application From Source Code

. In the *Project* screen, click on the *Browse Catalog* button to get the overview of all possible builder images.

. We choose `Ruby` here and click on *next*.
. Set the version to `2.2` 

. Specify the name and Git repository URL:
* *Name*: `my-ruby-hello-world`
* *Git Repository URL*: `https://github.com/openshift/ruby-hello-world`

. Click on *advanced options* and select the following options:
.. Notice that you get a route per default for your application.
.. Note that you can decide if Builds or Deployments should start automatically.
.. Change the scaling parameter to 3 replicas.
.. Add a label with the name `environment` and the value of `dev`.

. Click the blue *Create* button to create the application.

. Click *Continue to Overview* to go to the application's *Overview* screen.

. Click the *View Full Log* link on the right side to verify that a build is in progress.

. Review the log as the build progresses.

. Wait for the build to complete and go back to the overview page and click the blue external route link on the right side. It should look like `http://my-ruby-hello-world-my-ruby-project.apps-<GUID>.generic.opentlc.com`
.. The database for our application isn't running, so expect to see the webpage mention that.
+
[TIP]
====
* You can also use the command line to create a new application: `oc new-app https://github.com/openshift/ruby-hello-world -l  environment=dev`.

* To change scaling from the command line, use `oc scale`.
====

==== Scale Deployment 

. Go back to your application's *Overview* screen by clicking *Overview* at the upper left side.

. Click on the little `>` to expand the `deployment config`. 

. Observe the circle that shows the current number of pods, which is 3. You can increase that number by clicking the `^` button next to it.

. Click the `^` button twice to increase the number of replicas to 5.

. Go to *Applications* and select *Pods* to take a look at your new pods.

. Click on the name of one of your running *Pods*.

. Click on `Logs` to see the log for this Pod.

. Click on `Terminal` to open a shell console insde the Pod right in your webbrowser.

. Click on `Events` to see the last actions of this Pod.

. Go back to your application's *Overview* screen by clicking *Overview* again.

==== Create New Application through an Image

. As `student`, if you are not on the welcome screen, please click on the `OpenShift Container Platform` banner at the top left side.

. Click the blue *Create Project* button in the top right corner.

. Give the new project a name, display name, and description:
* *Name*: `parksmap`
* *Display Name*: `My Parksmap`
* *Description*: An interactive map.
. Click on the blue *Create* button.

Once the project is in place, click on your new project `My Parksmap`.

. Click on `Deploy Image`.

. Select `Image Name`.

. Use `openshiftroadshow/parksmap-katacoda:1.0.0` as image name and click on the magnifying glass.

. Click on `Deploy` and then on `Close`.

. Click on `Create Route` and then on `Create` for generating an external route to our new application.

. After that you can click on the link to our new app. It looks like `http://parksmap-katacoda-parksmap.apps-<GUID>.generic.opentlc.com`.

These are all the steps you need to run to get a "vanilla" Docker-formatted image deployed on OpenShift. This should work with any Docker-formatted image that follows best practices, such as defining the port any service is exposed on, not needing to run specifically as the root user or other dedicated user, and which embeds a default command for running the application.

=== Templates Lab

You can create a list of objects from a template using the CLI or, if a template has been uploaded to your project or the global template library, using the web console. For a curated set of templates, see the OpenShift Image Streams and Templates library. 

This lab includes the following sections:

* *Create and Upload Template*
+
In this section, you create a template for a two-tier application (front end and database), upload it into the shared namespace (the `openshift` project), and ensure that users can deploy it from the web console.

* *Use Templates and Template Parameters*
+
In this section, you create two separate template instances in two separate projects and establish a front-end-to-database-back-end connection by means of template parameters.

[NOTE] 
.Templates are a complex 
====
Templates allow an easy way to define all the required objects of an complex to be sepcified together and made available in Catalogs. Please see our https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html-single/developer_guide/#dev-guide-templates[OpenShift Documentation on Templates^] for more information.
====

==== Create and Upload Template

===== Install Template

The example in this section shows an application and a service with two pods: a front-end web tier and a back-end database tier. This application uses auto-generated parameters and other sleek features of OpenShift Container Platform.  Note that this application contains predefined connectivity between the front-end and back-end components as part of its YAML definition.

This example is, in effect, a "quick start" -- a predefined application that comes in a template and that you can immediately use or customize.

If not already, please login with `oc` as `admin` and change into the `default` project.
----
[root@workstation-<GUID> ~]# oc login https://master-prod.example.com:8443 -u admin -p openshift

...
----

Now we create a new project for the template based instant-app:
----
[root@workstation-<GUID> ~]# oc adm new-project instant-app --display-name="instant app example project" --description='A demonstration of an instant-app/template' --node-selector='region=primary' --admin=student

Created project instant-app
----

. Download the template's definition file:
+
----
[root@workstation-<GUID> ~]# wget http://www.rhpet.de/ocp-workshop/Template_Example.yml
----

. Create the template object in the `instant-app` project. This is also referred to as _uploading_ the template.
+
----
[root@workstation-<GUID> ~]# oc create -f Template_Example.yml -n instant-app

template "a-quickstart-keyvalue-application" created
----
NOTE: The `Template_Example.yml` file defines a template. You just added it to the instant-app project. If you add the template to the `openshift` project it made your template available throughout your OpenShift cluster for all projects.

The OpenShift Container Platform comes with a long list of preconfigured templates available for usage. You can take a look at the installed list with the following `oc` command. This list over 100 entries, that is why we did not include the output here. 

----
[root@workstation-<GUID> ~]# oc get templates -n openshift

NAME                                            DESCRIPTION                                                                        PARAMETERS        OBJECTS
3scale-gateway                                  3scale API Gateway                                                                 15 (6 blank)      2

... <many lines> ...

sso71-postgresql-persistent                     Application template for SSO 7.0 PostgreSQL applications with persistent storage   33 (17 blank)     9
----

Look for your generated template in your `instant-app` project:
----
[root@workstation-<GUID> ~]# oc get templates -n instant-app

NAME                                DESCRIPTION                                                         PARAMETERS        OBJECTS
a-quickstart-keyvalue-application   This is an example of a Ruby and MySQL application on OpenShift 3   5 (4 generated)   8
----

Do not be alarmed by the complexity of Templates. You can even create templates from existing Objects. Please see our Documentation on https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html-single/developer_guide/#export-as-template[How to Create a Template from existing Objects^].

===== Create Instant App from Template

. On your browser, connect to the OpenShift web console as `student` with the password `openshift`, if not already.
+
. If you are not on the welcome screen, please click on the `OpenShift Container Platform` banner at the top left side.
+
. Click on the link for the `instant app example project` at the top right.
+
. From the `instant app example project` project's *Overview* screen, click *Browse Catalog*.
+
Here you find the instant applications, a special kind of template with the `instant-app` tag. The idea behind an instant application is that, when you create a template instance, you already have a fully functional application. In this example, your instant application is just a simple web page for key-value storage and retrieval.
+
. Select *a-quickstart-keyvalue-application* and click on `Next`.
+
The template configuration screen is displayed. Here, you can specify certain options for instantiating the application components:
+
.. Set the `ADMIN_USERNAME` to `admin`.
.. Set the `ADMIN_PASSWORD` parameter to your favorite password.

. Click *Create* to instantiate the services, pods, replication controllers, etc. and then click on the `Continue to the project overview` link.

* The build starts immediately.
. Wait for the build to finish. You can browse the build logs to follow the progress.

[NOTE]
Our Application is currently still missing heath checks for all containers. If you are an experienced OpenShift User feel free to build a template with health checks included.

===== Use Application

After the build is complete and both frontend and database are up and running, visit your application at the `Routes - External Traffic` from the frontend deployment config. It should look like: `http://example-route-instant-app.apps-GUID.generic.opentlc.com`

[NOTE]
Be sure to use HTTP and _not_ HTTPS. HTTPS does not work for this example because the form submission was coded with HTTP links.

You see the same Frontend like in one of the labs before, but now with a running database :-)

For more informations about the powerfull template function, please look into the https://docs.openshift.com/container-platform/3.9/dev_guide/templates.html[template documentation^].

Thanks a lot for attending the *First steps in OpenShift Hands-on Lab*, we hope you enjoyed it.

For more cool, always accessible, hands-on labs for OpenShift, please visit https://learn.openshift.com[https://learn.openshift.com^].

If you are interested into a CI/CD hands-on lab, you can use your actual GUID for this http://devops-rhsummit.b9ad.pro-us-east-1.openshiftapps.com/index.html#/workshop/devops/module/devops-intro[Labguide^].
Please be aware that this Labguide is not tested with the environment, but it should work. Your environment will be up and running until 9pm today.

== Appendix: OpenShift Introduction

=== What is OpenShift ?

OpenShift Online is Red Hat’s public cloud application development and hosting platform that automates the provisioning, management and scaling of applications so that you can focus on writing the code for your business, startup, or big idea.

Official documentation for https://docs.openshift.com/container-platform/3.9/welcome/index.html[OpenShift Container Platform^]

=== Overview

OpenShift v3 is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as accurately as possible, with a focus on easy composition of applications by a developer. For example, install Ruby, push code, and add MySQL.

Unlike OpenShift v2, more flexibility of configuration is exposed after creation in all aspects of the model. The concept of an application as a separate object is removed in favor of more flexible composition of "services", allowing two web containers to reuse a database or expose a database directly to the edge of the network.

=== What are the Layers?

The Docker service provides the abstraction for packaging and creating Linux-based, lightweight https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/containers_and_images.html#containers[container images^]. Kubernetes provides the https://docs.openshift.com/container-platform/3.9/architecture/infrastructure_components/kubernetes_infrastructure.html#architecture-infrastructure-components-kubernetes-infrastructure[cluster management^] and orchestrates containers on multiple hosts.

OpenShift Container Platform adds:

* Source code management, https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/builds_and_image_streams.html#builds[builds^], and https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/deployments.html#architecture-core-concepts-deployments[deployments^] for developers

* Managing and promoting https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/containers_and_images.html#docker-images[images^] at scale as they flow through your system

* Application management at scale

* Team and user tracking for organizing a large developer organization

image::http://www.rhpet.de/pictures/OpenShift-Architecture.png[OpenShift Architecture]

=== What is the OCP Architecture?

OpenShift Container Platform has a microservices-based architecture of smaller, decoupled units that work together. It runs on top of a https://docs.openshift.com/container-platform/3.9/architecture/infrastructure_components/kubernetes_infrastructure.html#architecture-infrastructure-components-kubernetes-infrastructure[Kubernetes cluster^], with data about the objects stored in https://docs.openshift.com/container-platform/3.9/architecture/infrastructure_components/kubernetes_infrastructure.html#master[etcd^], a reliable clustered key-value store. Those services are broken down by function:

* https://docs.openshift.com/container-platform/3.9/rest_api/index.html#rest-api-index[REST APIs^], which expose each of the https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/index.html#architecture-core-concepts-index[core objects^].

* Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object.

Users make calls to the REST API to change the state of the system. Controllers use the REST API to read the user’s desired state, and then try to bring the other parts of the system into sync. For example, when a user requests a https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/builds_and_image_streams.html#builds[build^] they create a "build" object. The build controller sees that a new build has been created, and runs a process on the cluster to perform that build. When the build completes, the controller updates the build object via the REST API and the user sees that their build is complete.

The controller pattern means that much of the functionality in OpenShift Container Platform is extensible. The way that builds are run and launched can be customized independently of how images are managed, or how https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/deployments.html#architecture-core-concepts-deployments[deployments^] happen. The controllers are performing the "business logic" of the system, taking user actions and transforming them into reality. By customizing those controllers or replacing them with your own logic, different behaviors can be implemented. From a system administration perspective, this also means the API can be used to script common administrative actions on a repeating schedule. Those scripts are also controllers that watch for changes and take action. OpenShift Container Platform makes the ability to customize the cluster in this way a first-class behavior.

To make this possible, controllers leverage a reliable stream of changes to the system to sync their view of the system with what users are doing. This event stream pushes changes from etcd to the REST API and then to the controllers as soon as changes occur, so changes can ripple out through the system very quickly and efficiently. However, since failures can occur at any time, the controllers must also be able to get the latest state of the system at startup, and confirm that everything is in the right state. This resynchronization is important, because it means that even if something goes wrong, then the operator can restart the affected components, and the system double checks everything before continuing. The system should eventually converge to the user’s intent, since the controllers can always bring the system into sync.

=== Core Concepts

The following topics provide high-level, architectural information on core concepts and objects you will encounter when using OpenShift Container Platform. Many of these objects come from Kubernetes, which is extended by OpenShift Container Platform to provide a more feature-rich development lifecycle platform.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/containers_and_images.html#architecture-core-concepts-containers-and-images[Containers and images^] are the building blocks for deploying your applications.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/pods_and_services.html[Pods and services^] allow for containers to communicate with each other and proxy connections.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/projects_and_users.html[Projects and users^] provide the space and means for communities to organize and manage their content together.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/builds_and_image_streams.html[Builds and image streams^] allow you to build working images and react to new images.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/deployments.html[Deployments^] add expanded support for the software development and deployment lifecycle.

* https://docs.openshift.com/container-platform/3.9/architecture/networking/routes.html[Routes^] announce your service to the world.

* https://docs.openshift.com/container-platform/3.9/architecture/core_concepts/templates.html[Templates^] allow for many objects to be created at once based on customized parameters.

* https://docs.openshift.com/container-platform/3.9/architecture/additional_concepts/storage.html[Persistent Storage^] allow to save permanent data in a container.

Click on the links above if you want more information about the respective topic.

== Appendix: Lab Environment Information

You have nine VMs for your own use. Only the SSH Gateway and the Masternodes are reachable from the internet. All others can only be accessed through the SSH Gateway.

[cols="2,3,2,2", options="header"]
|===
| Name| external FQDN | internal FQDN | internal IP
| SSH Gateway | workstation-<GUID>.generic.opentlc.com | none | 192.168.0.5 & 192.168.1.5
| Dev Master | masterdev-<GUID>.generic.opentlc.com | master-dev.example.com | 192.168.1.10
| Dev Compute Node 1 | none | node01dev.example.com | 192.168.1.11
| Dev Compute Node 2 | none | node02dev.example.com | 192.168.1.12
| Dev Compute Node 3 | none | node03dev.example.com | 192.168.1.13
| Prod Master | masterprod-<GUID>.generic.opentlc.com | master-prod.example.com | 192.168.0.10
| Prod Compute Node 1 | none | node01prod.example.com | 192.168.0.11
| Prod Compute Node 2 | none | node02prod.example.com | 192.168.0.12
| Prod Compute Node 3 | none | node03prod.example.com | 192.168.0.13
|===

[cols="3*", options="header"]
|===
| Name | Password | Role
| root | <ask the instructor> | root user for all VMs
| admin | openshift | OSCP Administrator
| student | openshift | Developer & GIT User 
|=== 

*Wildcard FQDN for apps in the OpenShift Prod environment:* `*.apps-<GUID>.generic.opentlc.com`

WARNING: There is no public DNS resolution for apps for the OpenShift Dev environment.

=== Additional Services

*GIT Server*

Web: http://gogs-lab-infra.devapps-<GUID>.generic.opentlc.com

Username: `student` +
Password: `openshift`

*Nexus Maven Repository*

Web: http://nexus-lab-infra.devapps-<GUID>.generic.opentlc.com